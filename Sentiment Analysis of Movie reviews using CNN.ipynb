{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mqxxv2mo7tG1"
   },
   "source": [
    "<h1>Sentiment analysis using CNN</h1>\n",
    "<p>Copyright : Paritosh Morparia</p>\n",
    "<p>Indiana University</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data Source</h4>\n",
    "<p>The data used here is provided by keras as [IMDB movie reviews](https://keras.io/datasets/), where reviews have been classified as either positive or negative</p>\n",
    "<p>The data is available to import using the function:</p>\n",
    "<b>keras.datasets.imdb.load_data()</b></br>\n",
    "\n",
    "<p> The reasons of using this dataset are:<p>\n",
    "<ul>\n",
    "    <li>It has 50000 reviews</li>\n",
    "    <li>It is easy to use as the data has been transformed to a unique ndarray containing numerical values</li>\n",
    "    <li>Little amount of preprocessing is involved</li>\n",
    "</ul>\n",
    "\n",
    "<h5>The results of this experiment are compared with RNN in the next notebook</h5> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Fetching the data from keras</h4>\n",
    "<ul><li><p>It gives data in a numpy array</p></ul></li>\n",
    "<h4>Padding the data after fetching it</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IsMXj0Is5lUB"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=10000,\n",
    "                                                      skip_top=10,\n",
    "                                                      maxlen=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "y4QBiB-KUrJx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train2=np.zeros((25000,1000),dtype='int')\n",
    "x_test2=np.zeros((25000,1000),dtype='int')\n",
    "\n",
    "for i,x in enumerate(x_train):\n",
    "  x_train2[i]=np.asarray(np.pad(x,(0,1000-len(x)),\"constant\"))\n",
    "for i,x in enumerate(x_test):\n",
    "  x_test2[i]=np.asarray(np.pad(x,(0,1000-len(x)),\"constant\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Resizing the classes in one hot form for training the vector</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "n4VEZQH0LSY6"
   },
   "outputs": [],
   "source": [
    "y_train2=np.zeros((25000,2),dtype='int')\n",
    "y_test2=np.zeros((25000,2),dtype='int')\n",
    "\n",
    "for i,x in enumerate(y_train):\n",
    "  y_train2[i][x]=1\n",
    "for i,x in enumerate(y_test):\n",
    "  y_test2[i][x]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m2Gwp9w9EWnU"
   },
   "outputs": [],
   "source": [
    "from keras import regularizers,backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding, Flatten,Reshape\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Defining the archirecture of the model and setting it to train</h4>\n",
    "<p>The architecture comprises of following layers\n",
    "    <ul>\n",
    "        <li>Embedding layer          - 30 Nodes</li>\n",
    "        <li>3 Convolutional layers of size- 32,64 and 128 Kernels</li>\n",
    "        <li>Corresponding Pooling layers</li>\n",
    "        <li>Dense Layer</li>\n",
    "        <li>Softmax Layer</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>Other hyperparameters that were tweaked for the following net are\n",
    "    <ul>\n",
    "        <li>Activation        = Relu</li>\n",
    "        <li>Number of words in vocabulary</li>\n",
    "        <li>Loss function = categorical cross entropy</li>\n",
    "        <li>optimizer     = Adam optimizer</li>\n",
    "        <li>Number of Epochs</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 211200,
     "status": "ok",
     "timestamp": 1523847548364,
     "user": {
      "displayName": "paritosh morparia",
      "photoUrl": "//lh5.googleusercontent.com/-WJwnJ7gYK1I/AAAAAAAAAAI/AAAAAAAAACw/SwtXtxEEeXc/s50-c-k-no/photo.jpg",
      "userId": "104337524893826431305"
     },
     "user_tz": 240
    },
    "id": "SsNI9O2N5nQG",
    "outputId": "06655f78-1c7c-4126-fe75-81ba5b3e3fa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 21s 834us/step - loss: 0.1362 - acc: 0.7879 - val_loss: 0.0858 - val_acc: 0.8793\n",
      "Epoch 2/10\n",
      "20640/25000 [=======================>......] - ETA: 2s - loss: 0.0625 - acc: 0.919025000/25000 [==============================] - 21s 823us/step - loss: 0.0622 - acc: 0.9196 - val_loss: 0.0827 - val_acc: 0.8837\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 21s 824us/step - loss: 0.0411 - acc: 0.9490 - val_loss: 0.0919 - val_acc: 0.8735\n",
      "Epoch 4/10\n",
      " 9824/25000 [==========>...................] - ETA: 9s - loss: 0.0298 - acc: 0.964725000/25000 [==============================] - 21s 821us/step - loss: 0.0323 - acc: 0.9616 - val_loss: 0.0964 - val_acc: 0.8758\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 20s 820us/step - loss: 0.0260 - acc: 0.9690 - val_loss: 0.0978 - val_acc: 0.8731\n",
      "Epoch 6/10\n",
      " 7296/25000 [=======>......................] - ETA: 11s - loss: 0.0168 - acc: 0.980725000/25000 [==============================] - 21s 820us/step - loss: 0.0214 - acc: 0.9751 - val_loss: 0.1064 - val_acc: 0.8686\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 21s 821us/step - loss: 0.0203 - acc: 0.9766 - val_loss: 0.1025 - val_acc: 0.8755\n",
      "Epoch 8/10\n",
      " 6752/25000 [=======>......................] - ETA: 11s - loss: 0.0201 - acc: 0.977625000/25000 [==============================] - 20s 814us/step - loss: 0.0179 - acc: 0.9798 - val_loss: 0.1054 - val_acc: 0.8730\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 20s 818us/step - loss: 0.0149 - acc: 0.9832 - val_loss: 0.1161 - val_acc: 0.8644\n",
      "Epoch 10/10\n",
      " 6432/25000 [======>.......................] - ETA: 11s - loss: 0.0112 - acc: 0.987625000/25000 [==============================] - 20s 817us/step - loss: 0.0136 - acc: 0.9849 - val_loss: 0.1049 - val_acc: 0.8726\n",
      "25000/25000 [==============================] - 5s 198us/step\n",
      "Test score: 0.105, accuracy: 0.873\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 30\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "vocab_size=10002\n",
    "MAX_SENTENCE_LENGTH=1000\n",
    "\n",
    "model = Sequential()\n",
    "backend.clear_session()\n",
    "\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE,input_length=MAX_SENTENCE_LENGTH))\n",
    "\n",
    "model.add(Conv1D(32, 5, activation='relu',padding=\"same\", input_shape=(MAX_SENTENCE_LENGTH,EMBEDDING_SIZE)))\n",
    "# model.add(Conv1D(32, 5, activation='relu',padding=\"same\"))\n",
    "model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid'))\n",
    "\n",
    "model.add(Conv1D(64, 5, activation='relu',padding=\"same\"))\n",
    "# model.add(Conv1D(64, 5, activation='relu',padding=\"valid\"))\n",
    "model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu',padding=\"same\"))\n",
    "# model.add(Conv1D(128,kernel_size =5, activation=\"relu\", padding=\"valid\"))\n",
    "model.add(MaxPooling1D(pool_size=2, strides=None, padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train2, y_train2, epochs=NUM_EPOCHS,validation_data=[x_test2,y_test2])\n",
    "score,acc = model.evaluate(x_test2, y_test2)          \n",
    "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "7KSiIQFUSXLD"
   },
   "source": [
    "<h2>Results and analysis</h2>\n",
    "\n",
    "<p><b>Test Accuracy :-</b>87%</p>\n",
    "<p><b>Evaluation methods</b><br>\n",
    "Used mean squared error of the output classification to calculate the loss and evaluation while training. While validation, the output class was compared with the actual class.\n",
    "</p>\n",
    "\n",
    "\n",
    "<p>\n",
    "    <h3>Observations</h3>\n",
    "    <ul>\n",
    "        <li>The CNN network worked really quick on a large window size</li>\n",
    "        <li>On a intutive level, it seems that each convolution would try to group similar words together based on features such as parts of speech, dependencies and named entities on the given window size.\n",
    "    </ul>\n",
    "</p>\n",
    "<p><b>Experiments with the Architecture</b> \n",
    "<ul>\n",
    "    <li>Tried to train the network using more Convilutional layers, but it gave the same results. This might indicate that additional convolutional layers do not extract new features that would help classify the data better.</li>\n",
    "    <li>Tried to compare sigmoid and softmax activation for various architectures of which softmax worked better most of the time.</li>\n",
    "    <li>Tried playing around with length of kernel of which 5 fit the best in the experiments</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<p>The experiment further continues in the RNN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "A3.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
