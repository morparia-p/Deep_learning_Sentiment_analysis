{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sentiment Analysis Using RNN (LSTM)</h1>\n",
    "<p>Copyright : Paritosh Morparia</p>\n",
    "<p>Indiana University</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data Source</h4>\n",
    "<p>The data used here is provided by keras as [IMDB movie reviews](https://keras.io/datasets/), where reviews have been classified as either positive or negative</p>\n",
    "<p>The data is available to import using the function:</p>\n",
    "<b>keras.datasets.imdb.load_data()</b></br>\n",
    "\n",
    "<p> The reasons of using this dataset are:<p>\n",
    "<ul>\n",
    "    <li>It has 50000 reviews</li>\n",
    "    <li>It is easy to use as the data has been transformed to a unique ndarray containing numerical values</li>\n",
    "    <li>Little amount of preprocessing is involved</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>There is a really good example of [Movie reviewes using LSTM](https://github.com/keras-team/keras/edit/master/examples/imdb_lstm.py) which I used as a referene for  this assignment.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Fetching the data from keras</h4>\n",
    "<ul><li><p>It gives data in a numpy array</p></ul></li>\n",
    "<h4>Padding the data after fetching it</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "t0c3wIYEbG31"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=5000, #Vocab_size\n",
    "                                                      skip_top=10,\n",
    "                                                      maxlen=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Shuffeling and splitting the train and test data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "3PCLlmuCCp85"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=np.append(x_train,x_test)\n",
    "y=np.append(y_train,y_test)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x , y, test_size=0.33, random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "TpscHd2IEaIh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train2=np.zeros((len(x_train),200),dtype='int')\n",
    "x_test2=np.zeros((len(x_test),200),dtype='int')\n",
    "\n",
    "for i,x in enumerate(x_train):\n",
    "  x_train2[i]=np.asarray(np.pad(x,(0,200-len(x)),\"constant\"))\n",
    "for i,x in enumerate(x_test):\n",
    "  x_test2[i]=np.asarray(np.pad(x,(0,200-len(x)),\"constant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "GrWyIQK0EfSe"
   },
   "outputs": [],
   "source": [
    "y_train2=np.zeros((len(y_train),2),dtype='int')\n",
    "y_test2=np.zeros((len(y_test),2),dtype='int')\n",
    "\n",
    "for i,x in enumerate(y_train):\n",
    "  y_train2[i][x]=1\n",
    "for i,x in enumerate(y_test):\n",
    "  y_test2[i][x]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1523859200127,
     "user": {
      "displayName": "paritosh morparia",
      "photoUrl": "//lh5.googleusercontent.com/-WJwnJ7gYK1I/AAAAAAAAAAI/AAAAAAAAACw/SwtXtxEEeXc/s50-c-k-no/photo.jpg",
      "userId": "104337524893826431305"
     },
     "user_tz": 240
    },
    "id": "FvDqQJcWcBQi",
    "outputId": "6e20e79e-1ccc-4748-ada9-b7262071db06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19371, 200)\n"
     ]
    }
   ],
   "source": [
    "print((x_train2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "hPNNgN-7Ehep"
   },
   "outputs": [],
   "source": [
    "from keras import regularizers,backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding, Flatten,Reshape\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, SimpleRNN,LSTM\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Defining the archirecture of the model and setting it to train</h4>\n",
    "<p>The architecture comprises of following layers\n",
    "    <ol>\n",
    "        <li>Embedding layer - 128 Nodes</li>\n",
    "        <li>LSTM layer      - 128 Nodes</li>\n",
    "        <li>Dense layer     - 2 Nodes(Classes)</li>\n",
    "    </ol>\n",
    "</p>\n",
    "<p>Other hyperparameters that were tweaked for the following net are\n",
    "    <ul>\n",
    "        <li>Dropout Rate</li>\n",
    "        <li>Number of words in vocabulary</li>\n",
    "        <li>Loss function = binary cross entropy</li>\n",
    "        <li>optimizer     = Adam optimizer</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 535,
     "status": "ok",
     "timestamp": 1523860488678,
     "user": {
      "displayName": "paritosh morparia",
      "photoUrl": "//lh5.googleusercontent.com/-WJwnJ7gYK1I/AAAAAAAAAAI/AAAAAAAAACw/SwtXtxEEeXc/s50-c-k-no/photo.jpg",
      "userId": "104337524893826431305"
     },
     "user_tz": 240
    },
    "id": "IcxG6Lh2Ej_A",
    "outputId": "576b638a-b565-468c-b256-436a2a5a34dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 100)          500200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 580,802\n",
      "Trainable params: 580,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS =20\n",
    "vocab_size=5002\n",
    "MAX_SENTENCE_LENGTH=200\n",
    "backend.clear_session()\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,\n",
    "                    EMBEDDING_SIZE,\n",
    "                    input_length=MAX_SENTENCE_LENGTH,\n",
    "                   ))\n",
    "\n",
    "model.add(LSTM(HIDDEN_SIZE))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                 optimizer=optimizers.Adam(lr=0.01),\n",
    "               metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cellView": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1693967,
     "status": "ok",
     "timestamp": 1523862183839,
     "user": {
      "displayName": "paritosh morparia",
      "photoUrl": "//lh5.googleusercontent.com/-WJwnJ7gYK1I/AAAAAAAAAAI/AAAAAAAAACw/SwtXtxEEeXc/s50-c-k-no/photo.jpg",
      "userId": "104337524893826431305"
     },
     "user_tz": 240
    },
    "id": "2FERMCAMBv0E",
    "outputId": "54caafc6-7959-4f4f-93ad-47d2efad7140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19371 samples, validate on 9542 samples\n",
      "Epoch 1/20\n",
      "19371/19371 [==============================] - 84s 4ms/step - loss: 0.6938 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.4984\n",
      "Epoch 2/20\n",
      "19371/19371 [==============================] - 82s 4ms/step - loss: 0.6942 - acc: 0.5092 - val_loss: 0.6939 - val_acc: 0.5018\n",
      "Epoch 3/20\n",
      "10496/19371 [===============>..............] - ETA: 33s - loss: 0.6937 - acc: 0.495319371/19371 [==============================] - 83s 4ms/step - loss: 0.6849 - acc: 0.5235 - val_loss: 0.6423 - val_acc: 0.6416\n",
      "Epoch 4/20\n",
      "19371/19371 [==============================] - 83s 4ms/step - loss: 0.5617 - acc: 0.7287 - val_loss: 0.4612 - val_acc: 0.7955\n",
      "Epoch 5/20\n",
      "19371/19371 [==============================] - 82s 4ms/step - loss: 0.4257 - acc: 0.8123 - val_loss: 0.4410 - val_acc: 0.8117\n",
      "Epoch 6/20\n",
      " 1280/19371 [>.............................] - ETA: 1:10 - loss: 0.3813 - acc: 0.851619371/19371 [==============================] - 83s 4ms/step - loss: 0.3545 - acc: 0.8511 - val_loss: 0.3855 - val_acc: 0.8267\n",
      "Epoch 7/20\n",
      "19371/19371 [==============================] - 83s 4ms/step - loss: 0.3060 - acc: 0.8717 - val_loss: 0.3739 - val_acc: 0.8423\n",
      "Epoch 8/20\n",
      "19200/19371 [============================>.] - ETA: 0s - loss: 0.2883 - acc: 0.883919371/19371 [==============================] - 83s 4ms/step - loss: 0.2878 - acc: 0.8840 - val_loss: 0.3550 - val_acc: 0.8528\n",
      "Epoch 9/20\n",
      "19371/19371 [==============================] - 83s 4ms/step - loss: 0.2574 - acc: 0.8954 - val_loss: 0.3398 - val_acc: 0.8557\n",
      "Epoch 10/20\n",
      "19371/19371 [==============================] - 83s 4ms/step - loss: 0.2340 - acc: 0.9077 - val_loss: 0.3451 - val_acc: 0.8646\n",
      "Epoch 11/20\n",
      " 2688/19371 [===>..........................] - ETA: 1:04 - loss: 0.2182 - acc: 0.912619371/19371 [==============================] - 83s 4ms/step - loss: 0.2207 - acc: 0.9132 - val_loss: 0.3387 - val_acc: 0.8619\n",
      "Epoch 12/20\n",
      "19371/19371 [==============================] - 83s 4ms/step - loss: 0.2075 - acc: 0.9206 - val_loss: 0.3568 - val_acc: 0.8607\n",
      "Epoch 13/20\n",
      "19371/19371 [==============================] - 83s 4ms/step - loss: 0.1921 - acc: 0.9265 - val_loss: 0.3494 - val_acc: 0.8634\n",
      "Epoch 14/20\n",
      "19371/19371 [==============================] - 83s 4ms/step - loss: 0.1699 - acc: 0.9372 - val_loss: 0.3653 - val_acc: 0.8669\n",
      "Epoch 15/20\n",
      "15104/19371 [======================>.......] - ETA: 16s - loss: 0.1689 - acc: 0.936819371/19371 [==============================] - 83s 4ms/step - loss: 0.1719 - acc: 0.9351 - val_loss: 0.3618 - val_acc: 0.8628\n",
      "Epoch 16/20\n",
      "19371/19371 [==============================] - 83s 4ms/step - loss: 0.1519 - acc: 0.9447 - val_loss: 0.4035 - val_acc: 0.8542\n",
      "Epoch 17/20\n",
      "19371/19371 [==============================] - 83s 4ms/step - loss: 0.1431 - acc: 0.9470 - val_loss: 0.3817 - val_acc: 0.8658\n",
      "Epoch 18/20\n",
      " 1920/19371 [=>............................] - ETA: 1:06 - loss: 0.1086 - acc: 0.960919371/19371 [==============================] - 83s 4ms/step - loss: 0.1260 - acc: 0.9557 - val_loss: 0.4098 - val_acc: 0.8619\n",
      "Epoch 19/20\n",
      "19371/19371 [==============================] - 83s 4ms/step - loss: 0.1159 - acc: 0.9588 - val_loss: 0.4133 - val_acc: 0.8646\n",
      "Epoch 20/20\n",
      "19200/19371 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.956319371/19371 [==============================] - 82s 4ms/step - loss: 0.1225 - acc: 0.9560 - val_loss: 0.4347 - val_acc: 0.8622\n",
      "9542/9542 [==============================] - 37s 4ms/step\n",
      "Test score: 0.435, accuracy: 0.862\n"
     ]
    }
   ],
   "source": [
    "#@title Default title text\n",
    "model.fit(x_train2, y_train2, epochs=NUM_EPOCHS,validation_data=[x_test2,y_test2] ,batch_size= 128)\n",
    "score,acc = model.evaluate(x_test2, y_test2)          \n",
    "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1523848998945,
     "user": {
      "displayName": "paritosh morparia",
      "photoUrl": "//lh5.googleusercontent.com/-WJwnJ7gYK1I/AAAAAAAAAAI/AAAAAAAAACw/SwtXtxEEeXc/s50-c-k-no/photo.jpg",
      "userId": "104337524893826431305"
     },
     "user_tz": 240
    },
    "id": "dpLtsavoefw4",
    "outputId": "3bce56d5-9ab7-4af8-f1dd-9c92e45ead92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 50)           250100    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 272,952\n",
      "Trainable params: 272,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "YI89u0z9pHEA"
   },
   "source": [
    "<h2>Results and analysis</h2>\n",
    "\n",
    "<p><b>Test Accuracy :-</b>86.28%</p>\n",
    "<p><b>Evaluation methods</b><br>\n",
    "Used bonary crossentropy of the output classification to calculate the loss and evaluation while training. While validation, the output class was compared with the actual class.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <h3>Observations</h3>\n",
    "    <ul>\n",
    "        <li>The RNN network was really slow for huge sentence length. Cutting the sentence length to 200 worked best without affecting the results.</li>\n",
    "        <li>The RNN seems to train the data on previous values so develop a relationship with previous data</li>\n",
    "        <li>The training was slow in the beginning and was taking time to train. Changing the learning rate from 0.001 to 0.01 helped the speed of the network really well.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p><b>Experiments with the Architecture</b> \n",
    "<ul>\n",
    "    <li>Hidden Size played important role as giving less number of neurons affected the proformance</li>\n",
    "    <li>Tried to compare sigmoid and softmax activation for various architectures of which softmax worked better most of the time.</li>\n",
    "    <li></li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<h4>Comparison between CNN and RNN</h4>\n",
    "<p>Ideally, according to my readings, RNN generally works well when data has dependencies and needs to use them. In our case the data was a simple classification task and picking negative or positive words from the data may classify the data well. In that case CNN would work well.</p>\n",
    "<p>The results in this experiment seem almost equal for RNN and CNN. I believe more data and fine-tuning the parameters would cause better results. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "A3_RNN.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
